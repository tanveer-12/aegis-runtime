# AEGIS Runtime

AEGIS Runtime is a modular, GPU-aware adaptive inference system for large language models (LLMs).

It dynamically adjusts inference parameters such as batch size and numerical precision to maximize throughput (tokens/sec) while preventing out-of-memory (OOM) errors.

This project is designed as a systems + ML engineering experiment focused on observability, performance control, and adaptive runtime behavior.

---

## ğŸš€ Objective

Build a GPU-aware inference runtime that:

- Maximizes tokens per second
- Prevents CUDA OOM crashes
- Adapts dynamically to workload conditions
- Provides real-time logging and observability
- Demonstrates modular runtime system design

---

## ğŸ§  Core Idea

Transformer-based LLM inference scales non-linearly with:

- Batch size
- Sequence length
- KV cache growth
- Precision (fp16 vs bf16)

Instead of using static inference configurations, AEGIS:

1. Monitors GPU memory usage
2. Tracks throughput (tokens/sec)
3. Detects OOM risks
4. Dynamically adjusts:
   - Batch size
   - Precision
   - Maximum sequence length
5. Logs every decision transparently

---

## ğŸ— Architecture Overview

```
```
Incoming Request
      â”‚
      â–¼
  Scheduler          â† decides when and in what order requests are processed
      â”‚
      â–¼
  Controller         â† applies current agent settings to the request
      â”‚
      â–¼
   Monitor           â† reads GPU state before and after inference
      â”‚
      â–¼
Agent Decision       â† evaluates monitor output, updates control variables
      â”‚
      â–¼
Model Inference      â† TinyLlama forward pass with current batch/precision/seqlen
      â”‚
      â–¼
Metrics Tracker      â† computes tokens/sec, latency, OOM flag
      â”‚
      â–¼
Logger + SQLite      â† persists every cycle's full record
      â”‚
      â–¼
Live Dashboard       â† reads SQLite, visualizes in real-time
```
All components are modular and observable.

---


## ğŸ“ Project Structure


```
aegis-runtime/
â”‚
â”œâ”€â”€ main.py                # Entry point
â”œâ”€â”€ config.py              # Global configuration
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py          # Load TinyLlama / Mistral
â”‚   â””â”€â”€ inference.py       # Forward pass logic
â”‚
â”œâ”€â”€ runtime/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ monitor.py         # GPU + system metrics
â”‚   â”œâ”€â”€ scheduler.py       # Batch & microbatch logic
â”‚   â”œâ”€â”€ agent.py           # Decision-making logic
â”‚   â””â”€â”€ controller.py      # Orchestrates runtime flow
â”‚
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tracker.py         # Collect latency, throughput
â”‚   â”œâ”€â”€ logger.py          # Structured logging
â”‚   â””â”€â”€ database.py        # PostgreSQL integration
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py             # FastAPI backend (metrics endpoint)
â”‚   â””â”€â”€ app.py             # Streamlit dashboard
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ benchmark.py       # Controlled test scenarios
â”‚
â””â”€â”€ logs/
    â””â”€â”€ runtime.log
```
---

## âš™ï¸ Model

Default model: TinyLlama/TinyLlama-1.1B-Chat-v1.0


- 1.1B parameters
- Transformer-based
- Supports fp16 / bf16
- Suitable for GPU memory experimentation

---

## ğŸ“Š Observability & Logging

Every inference cycle logs:

- Timestamp
- Batch size
- Sequence length
- Precision
- Tokens generated
- Tokens/sec
- Latency
- GPU memory allocated
- GPU memory peak
- GPU utilization %
- Agent decision reason
- OOM events

Logs are stored in:

- Console (real-time)
- SQLite database
- Live dashboard visualizations

---

## ğŸ¯ Adaptive Agent Rules (Initial Version)

- If GPU memory < 60% â†’ Increase batch size
- If GPU memory > 85% â†’ Reduce batch size
- If OOM occurs â†’ Halve batch size and retry
- If memory pressure high â†’ Switch to fp16
- If long sequence degrades throughput â†’ Cap max sequence length

All decisions are logged and observable.

---
## ğŸ§ª Benchmark Goals

Evaluate:

- Tokens/sec vs batch size
- Memory usage vs sequence length
- Adaptive runtime vs static configuration
- OOM prevention effectiveness

---

## ğŸ–¥ Environment

- WSL2 + Ubuntu 20.04
- Python 3.10+
- CUDA-enabled PyTorch
- Conda environment
- SQLite for metrics storage
- Streamlit or FastAPI for dashboard

---

## ğŸ“ˆ Why This Project Matters

AEGIS demonstrates:

- Understanding of GPU memory behavior
- Transformer inference scaling
- Runtime system design
- Observability-driven engineering
- Adaptive decision logic in ML systems
- ## ğŸ”® Future Extensions

- Replace SQLite with PostgreSQL for distributed deployment
- Multi-GPU scheduling
- Reinforcement learning-based agent
- Integration with vLLM or TensorRT
- Cloud deployment (AWS/GCP)
- Distributed request queue

---

## ğŸ“Œ Status

Work in progress.

Currently focused on building the adaptive GPU-aware inference control layer and observability system.

---
## ğŸ‘¤ Author

Tanveer  
MS Computer Science  
Focus: Computer Science + ML Engineering
